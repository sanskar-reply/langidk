{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloudpickle is not installed. Please call `pip install google-cloud-aiplatform[preview]`.\n"
     ]
    }
   ],
   "source": [
    "chat=ChatVertexAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" J'aime la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that translates English to French.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French. I love programming.\"\n",
    "    ),\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a gruesome joke about goats.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"gruesome\", content=\"goats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "8 validation errors for ChatPromptTemplate\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 1\n  value is not a valid dict (type=type_error.dict)\nmessages -> 1\n  value is not a valid dict (type=type_error.dict)\nmessages -> 2\n  value is not a valid dict (type=type_error.dict)\nmessages -> 2\n  value is not a valid dict (type=type_error.dict)\nmessages -> 3\n  value is not a valid dict (type=type_error.dict)\nmessages -> 3\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompts\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m----> 3\u001b[0m template \u001b[39m=\u001b[39m ChatPromptTemplate\u001b[39m.\u001b[39;49mfrom_messages([\n\u001b[1;32m      4\u001b[0m     (\u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mYou are a helpful AI bot. Your name is \u001b[39;49m\u001b[39m{name}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      5\u001b[0m     (\u001b[39m\"\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mHello, how are you doing?\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      6\u001b[0m     (\u001b[39m\"\u001b[39;49m\u001b[39mai\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mI\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mm doing well, thanks!\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      7\u001b[0m     (\u001b[39m\"\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m{user_input}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      8\u001b[0m ])\n\u001b[1;32m     10\u001b[0m messages \u001b[39m=\u001b[39m template\u001b[39m.\u001b[39mformat_messages(\n\u001b[1;32m     11\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBob\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     user_input\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhat is your name?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(messages)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/prompts/chat.py:223\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[0;34m(cls, messages)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(message, BaseMessagePromptTemplate):\n\u001b[1;32m    222\u001b[0m         input_vars\u001b[39m.\u001b[39mupdate(message\u001b[39m.\u001b[39minput_variables)\n\u001b[0;32m--> 223\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(input_variables\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(input_vars), messages\u001b[39m=\u001b[39;49mmessages)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newwwwww/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 8 validation errors for ChatPromptTemplate\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 1\n  value is not a valid dict (type=type_error.dict)\nmessages -> 1\n  value is not a valid dict (type=type_error.dict)\nmessages -> 2\n  value is not a valid dict (type=type_error.dict)\nmessages -> 2\n  value is not a valid dict (type=type_error.dict)\nmessages -> 3\n  value is not a valid dict (type=type_error.dict)\nmessages -> 3\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' You have a very refined palate and prefer to eat healthy and nutritious foods.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "\n",
    "llm = ChatVertexAI()\n",
    "llm(template.format_messages(text='i dont like eating tasty things.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a offensive joke about racism'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {topic}\"\n",
    ")\n",
    "\n",
    "prompt_template.format(adjective=\"offensive\", topic=\"racism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring all the models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m             time\u001b[39m.\u001b[39msleep(sleep_time)\n\u001b[0;32m---> 18\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39mCustomVertexAIEmbeddings\u001b[39;49;00m(VertexAIEmbeddings, BaseModel):\n\u001b[1;32m     19\u001b[0m     requests_per_minute: \u001b[39mint\u001b[39;49m\n\u001b[1;32m     20\u001b[0m     num_instances_per_batch: \u001b[39mint\u001b[39;49m\n",
      "\u001b[0;31mTypeError\u001b[0m: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from pydantic import BaseModel\n",
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomVertexAIEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m EMBEDDING_QPM \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m     20\u001b[0m EMBEDDING_NUM_BATCH \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m---> 21\u001b[0m embeddings \u001b[39m=\u001b[39m CustomVertexAIEmbeddings(\n\u001b[1;32m     22\u001b[0m     requests_per_minute\u001b[39m=\u001b[39mEMBEDDING_QPM,\n\u001b[1;32m     23\u001b[0m     num_instances_per_batch\u001b[39m=\u001b[39mEMBEDDING_NUM_BATCH,\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomVertexAIEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison-32k\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Chat\n",
    "chat = ChatVertexAI(\n",
    "    model_name = \"chat-bison-32k\",\n",
    ")\n",
    "\n",
    "# Embedding\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token counting\n",
    "import asyncio\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "with get_openai_callback() as cb:\n",
    "    llm(\"What is the square root of 4?\")\n",
    "\n",
    "total_tokens = cb.total_tokens\n",
    "assert total_tokens > 0\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    llm(\"What is the square root of 4?\")\n",
    "    llm(\"What is the square root of 4?\")\n",
    "\n",
    "assert cb.total_tokens == total_tokens * 2\n",
    "\n",
    "# You can kick off concurrent runs from within the context manager\n",
    "with get_openai_callback() as cb:\n",
    "    await asyncio.gather(\n",
    "        *[llm.agenerate([\"What is the square root of 4?\"]) for _ in range(3)]\n",
    "    )\n",
    "\n",
    "assert cb.total_tokens == total_tokens * 3\n",
    "\n",
    "# The context manager is concurrency safe\n",
    "task = asyncio.create_task(llm.agenerate([\"What is the square root of 4?\"]))\n",
    "with get_openai_callback() as cb:\n",
    "    await llm.agenerate([\"What is the square root of 4?\"])\n",
    "\n",
    "await task\n",
    "assert cb.total_tokens == total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 1 + 2 = 3'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "# Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "chain.run(number=2)\n",
    "\n",
    "# Use verbose flag: Then, let's use the `verbose` flag to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "chain.run(number=2)\n",
    "\n",
    "# Request callbacks: Finally, let's use the request `callbacks` to achieve the same result\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(number=2, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='hello\\n\\n# What is your name?\\n', metadata={'source': './index.md'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document loaders\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./index.md\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web based loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "     \n",
    "\n",
    "loader = WebBaseLoader(\"https://cloud.google.com/blog/products/ai-machine-learning/generative-ai-applications-with-vertex-ai-palm-2-models-and-langchain\")     \n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 comments\n",
      "Here's a sample:\n",
      "\n",
      "Generative AI applications with Vertex AI PaLM 2 Models and LangChain | Google Cloud BlogJump to ContentCloudBlogContact sales Get started for free CloudBlogSolutions & technologyAI & Machine LearningAPI ManagementApplication DevelopmentApplication ModernizationChrome EnterpriseComputeContainers & KubernetesData AnalyticsDatabasesDevOps & SREMaps & GeospatialSecurity & IdentityInfrastructureInfrastructure ModernizationNetworkingProductivity & CollaborationSAP on Google CloudStorage & Data Transf\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(data)} comments\")\n",
    "print(f\"Here's a sample:\\n\\n{''.join([x.page_content[:500] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the capital of Spain?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the capital of Spain?\n",
      "AI:  The capital of Spain is Madrid. It is located in the center of the country and is the largest city in Spain with a population of over 3 million people.\n",
      "Human: What are some popular places I can see in France?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: What is the capital of Spain?\n",
      "AI:  The capital of Spain is Madrid. It is located in the center of the country and is the largest city in Spain with a population of over 3 million people.\n",
      "Human: What are some popular places I can see in France?\n",
      "AI:  Some popular places to visit in France include the Eiffel Tower in Paris, the Palace of Versailles, the Louvre Museum, the Mont Saint-Michel, and the Château de Chambord.\n",
      "Human: What question did I ask first?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You asked what the capital of Spain is.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")\n",
    "     \n",
    "\n",
    "conversation.predict(input=\"What is the capital of Spain?\")\n",
    "     \n",
    "\n",
    "conversation.predict(input=\"What are some popular places I can see in France?\")\n",
    "     \n",
    "\n",
    "conversation.predict(input=\"What question did I ask first?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mA classic dish from Mumbai is Pav Bhaji, a spicy vegetable curry served with a soft bread roll.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mPav Bhaji Recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 teaspoon cumin seeds\n",
      "- 1 onion, finely chopped\n",
      "- 2 cloves garlic, minced\n",
      "- 1 teaspoon ginger, minced\n",
      "- 1 green chili, finely chopped\n",
      "- 1 teaspoon turmeric powder\n",
      "- 1 teaspoon coriander powder\n",
      "- 1 teaspoon garam masala\n",
      "- 2 large potatoes, peeled and diced\n",
      "- 1 large carrot, peeled and diced\n",
      "- 1 cup green peas\n",
      "- 1 cup cauliflower florets\n",
      "- 1 cup tomato puree\n",
      "- Salt to taste\n",
      "- 2 tablespoons butter\n",
      "- Pav buns\n",
      "\n",
      "Instructions:\n",
      "1. Heat the oil in a large pan over medium heat.\n",
      "2. Add the cumin seeds and let them sizzle for a few seconds.\n",
      "3. Add the onion, garlic, ginger, and green chili and sauté until the onion is softened.\n",
      "4. Add the turmeric, coriander, and garam masala and stir to combine.\n",
      "5. Add the potatoes, carrot, green peas, and cauliflower and stir to combine.\n",
      "6. Add the tomato puree and salt and stir to combine.\n",
      "7. Cover the pan and let\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
    "\n",
    "review = overall_chain.run(\"Mumbai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarisation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in the document = 13656\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"How to use Grounding for your LLMs with text embeddings | Google Cloud BlogJump to ContentCloudBlogContact sales Get started for free CloudBlogSolutions & technologyAI & Machine LearningAPI ManagementApplication DevelopmentApplication ModernizationChrome EnterpriseComputeContainers & KubernetesData AnalyticsDatabasesDevOps & SREMaps & GeospatialSecurity & IdentityInfrastructureInfrastructure ModernizationNetworkingProductivity & CollaborationSAP on Google CloudStorage & Data TransferSustainabilityEcosystemIT LeadersIndustriesFinancial ServicesHealthcare & Life SciencesManufacturingMedia & EntertainmentPublic SectorRetailSupply ChainTelecommunicationsPartnersStartups & SMBTraining & CertificationsInside Google CloudGoogle Cloud Next & EventsGoogle Maps PlatformGoogle WorkspaceDevelopers & PractitionersTransform with Google CloudContact sales Get started for free AI & Machine LearningVertex AI Embeddings for Text: Grounding LLMs made easyMay 26, 2023Kaz SatoDeveloper Advocate, Google CloudIvan CheungDeveloper Programs Engineer, Google CloudGoogle Cloud NextView the latest announcements.Register Many people are now starting to think about how to bring Gen AI and large language models (LLMs) to production services. You may be wondering \"How to integrate LLMs or AI chatbots with existing IT systems, databases and business data?\", \"We have thousands of products. How can I let LLM memorize them all precisely?\", or \"How to handle the hallucination issues in AI chatbots to build a\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"hallucination issues in AI chatbots to build a reliable service?\". Here is a quick solution: grounding with embeddings and vector search.What is grounding? What are embedding and vector search? In this post, we will learn these crucial concepts to build reliable Gen AI services for enterprise use. But before we dive deeper, here is an example:Semantic search on 8 million Stack Overflow questions in milliseconds. (Try the demo here)This demo is available as a public live demo here. Select \"STACKOVERFLOW\" and enter any coding question as a query, so it runs a text search on 8 million questions posted on Stack Overflow.The following points make this demo unique:LLM-enabled semantic search: The 8 million Stack Overflow questions and query text are both interpreted by Vertex AI Generative AI models. The model understands the meaning and intent (semantics) of the text and code snippets in the question body at librarian-level precision. The demo leverages this ability for finding highly relevant questions and goes far beyond simple keyword search in terms of user experience. For example, if you enter \"How do I write a class that instantiates only once\", then the demo shows \"How to create a singleton class\" at the top, as the model knows their meanings are the same in the context of computer programming.Grounded to business facts: In this demo, we didn't try having the LLM to memorize the 8 million items with complex and lengthy prompt engineering. Instead, we attached the Stack\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"engineering. Instead, we attached the Stack Overflow dataset to the model as an external memory using vector search, and used no prompt engineering. This means, the outputs are all directly \"grounded\" (connected) to the business facts, not the artificial output from the LLM. So the demo is ready to be served today as a production service with mission critical business responsibility. It does not suffer from the limitation of LLM memory or unexpected behaviors of LLMs such as the hallucinations.Scalable and fast: The demo gives you the search results in tens of milliseconds while retaining the deep semantic understanding capability. Also, the demo is capable of scaling out to handle thousands of search queries every second. This is enabled with the combination of LLM embeddings and Google AI's vector search technology.The key enablers of this solution are 1) the embeddings generated with Vertex AI Embeddings for Text and 2) fast and scalable vector search by Vertex AI Matching Engine. Let's start by taking a look at these technologies.First key enabler: Vertex AI Embeddings for TextOn May 10, 2023, Google Cloud announced the following Embedding APIs for Text and Image. They are available on Vertex AI Model Garden. Embeddings for Text : The API takes text input up to 3,072 input tokens and outputs 768 dimensional text embeddings, and is available as a public preview. As of May 10, 2023, the pricing is $0.0001 per 1000 characters (the latest pricing is available on the Pricing\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"(the latest pricing is available on the Pricing for Generative AI models page).Embeddings for Image: Based on Google AI's Contrastive Captioners (CoCa) model, the API takes either image or text input and outputs 1024 dimensional image/text multimodal embeddings, available to trusted testers. This API outputs so-called \"multimodal\" embeddings, enabling multimodal queries where you can execute semantic search on images by text queries, or vise-versa. We will feature this API in another blog post soon.In this blog, we will explain more about why embeddings are useful and show you how to build and an application leveraging Embeddings API for Text. In a future blog post, we will provide a deep dive on Embeddings API for Image.Embeddings API for Text on Vertex AI Model GardenWhat is embeddings?So, what are semantic search and embeddings? With the rise of LLMs, why is it becoming important for IT engineers and ITDMs to understand how they work? To learn it, please take a look at this video from a Google I/O 2023 session for 5 minutes:Also, Foundational courses: Embeddings on Google Machine Learning Crush Course and Meet AI’s multitool: Vector embeddings by Dale Markowitz are great materials to learn more about embeddings.LLM text embedding business use casesWith the embedding API, you can apply the innovation of embeddings, combined with the LLM capability, to various text processing tasks, such as:LLM-enabled Semantic Search: text embeddings can be used to represent both the\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"text embeddings can be used to represent both the meaning and intent of a user's query and documents in the embedding space. Documents that have similar meaning to the user's query intent will be found fast with vector search technology. The model is capable of generating text embeddings that capture the subtle nuances of each sentence and paragraphs in the document.LLM-enabled Text Classification: LLM text embeddings can be used for text classification with a deep understanding of different contexts without any training or fine-tuning (so-called zero-shot learning). This wasn't possible with the past language models without task-specific training.LLM-enabled Recommendation: The text embedding can be used for recommendation systems as a strong feature for training recommendation models such as Two-Tower model. The model learns the relationship between the query and candidate embeddings, resulting in next-gen user experience with semantic product recommendation.LLM-enabled Clustering, Anomaly Detection, Sentiment Analysis, and more, can be also handled with the LLM-level deep semantics understanding.Sorting 8 million texts at \"librarian-level\" precision Vertex AI Embeddings for Text has an embedding space with 768 dimensions. As explained in the video above, the space represents a huge map of a wide variety of texts in the world, organized by their meanings. With each input text, the model can find a location (embedding) in the map.The API can take 3,072 input tokens, so it\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"map.The API can take 3,072 input tokens, so it can digest the overall meaning of a long text and even programming code, and represent it as single embedding. It is like having a librarian knowledgeable about a wide variety of industries, reading through millions of texts carefully, and sorting them with millions of nano-categories that can classify even slight differences of subtle nuances.By visualizing the embedding space, you can actually observe how the model sorts the texts at the \"librarian-level\" precision. Nomic AI provides a platform called Atlas for storing, visualizing and interacting with embedding spaces with high scalability and in a smooth UI, and they worked with Google for visualizing the embedding space of the 8 million Stack Overflow questions. You can try exploring around the space, zooming in and out to each data point on your browser on this page, courtesy of Nomic AI.8 million Stack Overflow questions embedding spaceVisualized by Nomic AI Atlas (Try exploring it here)Examples of the \"librarian-level\" semantic understanding by Embeddings API with Stack Overflow questionsNote that this demo didn't require any training or fine-tuning with computer programming specific datasets. This is the innovative part of the zero-shot learning capability of the LLM; it can be applied to a wide variety of industries, including finance, healthcare, retail, manufacturing, construction, media, and more, for deep semantic search on the industry-focused business documents\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"search on the industry-focused business documents without spending time and cost for collecting industry specific datasets and training models.The second key enabler: fast and scalable Vector SearchThe second key enabler of the Stack Overflow demo shown earlier is the vector search technology. This is another innovation we are having in the data science field.The problem is \"how to find similar embeddings in the embedding space\". Since embeddings are vectors, this can be done by calculating the distance or similarity between vectors, as shown below.But this isn't easy when you have millions or billions of embeddings. For example, if you have 8 million embeddings with 768 dimensions, you would need to repeat the calculation in the order of 8 million x 768. This would take a very long time to finish. Actually, when we tried this on BigQuery with one million embeddings five years ago, it took 20 seconds.So the researchers have been studying a technique called Approximate Nearest Neighbor (ANN) for faster search. ANN uses \"vector quantization\" for separating the space into multiple spaces with a tree structure. This is similar to the index in relational databases for improving the query performance, enabling very fast and scalable search with billions of embeddings.With the rise of LLMs, the ANN is getting popular quite rapidly, known as the Vector Search technology.In 2020, Google Research published a new ANN algorithm called ScaNN. It is considered one of the best ANN\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"ScaNN. It is considered one of the best ANN algorithms in the industry, also the most important foundation for search and recommendation in major Google services such as Google Search, YouTube and many others.Google Cloud developers can take the full advantage of Google's vector search technology with Vertex AI Matching Engine. With this fully managed service, developers can just add the embeddings to its index and issue a search query with a key embedding for the blazingly fast vector search. In the case of the Stack Overflow demo, Matching Engine can find relevant questions from 8 million embeddings in tens of milliseconds.With Matching Engine, you don't need to spend much time and money building your own vector search service from scratch or using open source tools if your goal is high scalability, availability and maintainability for production systems.Grounding LLM outputs with Matching EngineBy combining the Embeddings API and Matching Engine, you can use the embeddings to \"ground\" LLM outputs to real business data with low latency:In the case of the Stack Overflow demo shown earlier, we've built a system with the following architecture.Stack Overflow semantic search demo architectureThe demo architecture has two parts: 1) building a Matching Engine index with Vertex AI Workbench and the Stack Overflow dataset on BigQuery (on the right) and 2) processing vector search requests with Cloud Run (on the left) and Matching Engine. For the details, please see the sample\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Engine. For the details, please see the sample Notebook on GitHub.Grounding LLMs with LangChain and Vertex AIIn addition to the architecture used for the Stack Overflow demo, another popular way for grounding is to enter the vector search result into the LLM and let the LLM generate the final answer text for the user. LangChain is a popular tool for implementing this pipeline, and Vertex AI Gen AI embedding APIs and Matching Engine are definitely best suited for LangChain integration. In a future blog post, we will explore this topic further. So stay tuned!How to get startedIn this post, we have seen how the combination of Embeddings for Text API  and Matching Engine allows enterprises to use Gen AI and LLMs in a grounded and reliable way. The fine-grained semantic understanding capability of the API can bring the intelligence to information search and recommendation in a wide variety of businesses, setting a new standard of user experience in enterprise IT systems.To get started, please check out the following resources:Stack Overflow semantic search demo: sample Notebook on GitHubVertex AI Embeddings for Text API documentationMatching Engine documentationAI & Machine LearningGoogle Cloud advances generative AI at I/O: new foundation models, embeddings, and tuning tools in Vertex AIBy June Yang • 5-minute readPosted inAI & Machine LearningDevelopers & PractitionersRelated articlesStartupsListenField enables farmers to harvest the benefits of AI and machine learningBy\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"harvest the benefits of AI and machine learningBy Rassarin Chinnachodteeranun • 4-minute readAI & Machine LearningHow Verve Group transforms customer experiences with Google Cloud Vertex AIBy Keelin McDonell • 2-minute readFinancial ServicesHow Resistant AI uses Document AI for fraud-resilient automated document processingBy Alfredo Dos Santos • 3-minute readPartnersBuilt with Google Cloud AI: How AI can improve data quality and observability for startups and enterprisesBy Max Lukichev • 4-minute readFooter LinksFollow usGoogle CloudGoogle Cloud ProductsPrivacyTermsHelpLanguage‪English‬‪Deutsch‬‪Français‬‪한국어‬‪日本語‬\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" This article discusses how to use Grounding for LLMs with text embeddings in order to integrate LLMs and AI chatbots with existing IT systems, databases, and business data. It also covers how to handle the hallucination issues in AI chatbots in order to build a reliable service.\n",
      "\n",
      " This post discusses the concept of grounding with embeddings and vector search to build reliable AI services for enterprise use. An example of this is a live demo of a semantic search on 8 million Stack Overflow questions in milliseconds. This demo is unique in that it leverages LLM-enabled semantic search to understand the meaning and intent of the text and code snippets in the question body. Additionally, the demo is grounded to business facts, meaning that the LLM does not need to memorize the 8 million items.\n",
      "\n",
      " Google Cloud announced the Vertex AI Model Garden, which includes Embeddings for Text and Image APIs. These APIs are used to attach the Stack Overflow dataset to the model as an external memory using vector search, allowing for fast and scalable search results in tens of milliseconds. The key enablers of this solution are Vertex AI Embeddings for Text and Vertex AI Matching Engine.\n",
      "\n",
      " Google AI's Contrastive Captioners (CoCa) model outputs 1024 dimensional image/text multimodal embeddings, which can be used for semantic search on images by text queries or vice versa. This blog explains why embeddings are useful and how to build an application leveraging Embeddings API for Text. LLM text embedding business use cases include LLM-enabled Semantic Search, text classification, and sentiment analysis.\n",
      "\n",
      "\n",
      "\n",
      "Vertex AI Embeddings for Text is a model that uses text embeddings to represent the meaning and intent of a user's query and documents in an embedding space. This allows for vector search technology to quickly find documents with similar meaning to the user's query intent. The model can also be used for text classification, recommendation systems, clustering, anomaly detection, sentiment analysis, and more. It has an embedding space with 768 dimensions and can take 3,072 input tokens.\n",
      "\n",
      " Nomic AI's Embeddings API is a zero-shot learning capability that can take 3,072 input tokens and represent them as a single embedding. It can be applied to a wide variety of industries for deep semantic search on business documents. Nomic AI's Atlas platform can store, visualize, and interact with embedding spaces with high scalability and a smooth UI. An example of this is the 8 million Stack Overflow questions embedding space, which can be explored on a browser.\n",
      "\n",
      " Vector Search technology is an innovation in the data science field that enables fast and scalable search of industry-focused business documents without spending time and cost for collecting industry specific datasets and training models. Approximate Nearest Neighbor (ANN) is a technique used for faster search, which uses vector quantization to separate the space into multiple spaces with a tree structure. In 2020, Google Research published a new ANN algorithm called ScaNN, which is considered one of the best ANN.\n",
      "\n",
      " ScaNN is a powerful ANN algorithm used in Google services such as Google Search and YouTube. Vertex AI Matching Engine is a fully managed service that allows developers to quickly add embeddings to an index and search for relevant data. The Stack Overflow demo combines the Embeddings API and Matching Engine to ground LLM outputs to real business data with low latency. The demo architecture consists of two parts: building a Matching Engine index and processing vector search requests.\n",
      "\n",
      " This article discusses how the combination of Embeddings for Text API and Matching Engine can be used to bring intelligence to information search and recommendation in a variety of businesses. It also introduces LangChain and Vertex AI Gen AI embedding APIs and Matching Engine as popular tools for grounding LLMs. Finally, it provides resources for developers and practitioners to get started.\n",
      "\n",
      " This article discusses how companies can use AI and Machine Learning to improve customer experiences, fraud-resilient automated document processing, and data quality and observability for startups and enterprises. It also provides links to Google Cloud products, privacy, terms, and language options.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" This article explains how to use Grounding for LLMs with text embeddings to integrate LLMs and AI chatbots with existing IT systems, databases, and business data. It covers how to handle the hallucination issues in AI chatbots in order to build a reliable service. It introduces Google Cloud's Vertex AI Model Garden, Nomic AI's Embeddings API, and Google Research's ScaNN algorithm as popular tools for grounding LLMs. It also provides resources for developers and practitioners to get started with AI and Machine Learning to improve customer experiences, fraud-resilient automated document processing, and data quality and observability.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    \"https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-with-text-embeddings\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"# of words in the document = {len(documents[0].page_content)}\")\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest PDF files\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load GOOG's 10K annual report (92 pages).\n",
    "url = \"https://abc.xyz/investor/static/pdf/20230203_alphabet_10K.pdf\"\n",
    "loader = PyPDFLoader(url)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents = 263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split the documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"# of documents = {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# select embedding engine - we use Vertex PaLM Embeddings API\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m embeddings\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# select embedding engine - we use Vertex PaLM Embeddings API\n",
    "embeddings="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Store docs in local vectorstore as index\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# it may take a while since API is rate limited\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m Chroma\n\u001b[0;32m----> 5\u001b[0m db \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39mfrom_documents(docs, embeddings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Store docs in local vectorstore as index\n",
    "# it may take a while since API is rate limited\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose index to the retriever\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain to answer questions\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Uses LLM to synthesize results from the search index.\n",
    "# We use Vertex PaLM Text API for LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was Alphabet's net income in 2022?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much office space reduction took place in 2023?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newwwwww",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
